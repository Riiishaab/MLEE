{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with 2D CNN"
      ],
      "metadata": {
        "id": "_U3VoTpDGMwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brFJRPEh_7HT",
        "outputId": "c85b3f6d-4125-461f-b6af-0ebdf379ebc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "import zipfile\n",
        "import os\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "F4-4I0AeBOJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the ZIP files\n",
        "path_normal = '/content/drive/MyDrive/Normal.zip'\n",
        "path_pneumonia = '/content/drive/MyDrive/Pneumonia.zip'\n",
        "path_tuberculosis = '/content/drive/MyDrive/Tuberculosis.zip'\n",
        "\n",
        "# Destination directories for extracted files\n",
        "output_normal = '/content/drive/MyDrive/Extracted/Normal/'\n",
        "output_pneumonia = '/content/drive/MyDrive/Extracted/Pneumonia/'\n",
        "output_tuberculosis = '/content/drive/MyDrive/Extracted/Tuberculosis/'\n",
        "\n",
        "# Function to extract ZIP files\n",
        "def extract_zip(zip_path, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)  # Create the directory if it doesn't exist\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "    print(f\"Extracted {zip_path} to {output_dir}\")\n",
        "\n",
        "# Extract the contents of each ZIP file\n",
        "extract_zip(path_normal, output_normal)\n",
        "extract_zip(path_pneumonia, output_pneumonia)\n",
        "extract_zip(path_tuberculosis, output_tuberculosis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFVpt51MAnfq",
        "outputId": "518761d4-9b54-4c10-d6f4-2536a23f56bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted /content/drive/MyDrive/Normal.zip to /content/drive/MyDrive/Extracted/Normal/\n",
            "Extracted /content/drive/MyDrive/Pneumonia.zip to /content/drive/MyDrive/Extracted/Pneumonia/\n",
            "Extracted /content/drive/MyDrive/Tuberculosis.zip to /content/drive/MyDrive/Extracted/Tuberculosis/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (128, 128))\n",
        "        img = img / 255.0  # Normalize pixel values to [0, 1]\n",
        "        img = np.transpose(img, (2, 0, 1))  # Convert to (C, H, W) format for PyTorch\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return torch.tensor(img, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Load images and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Class-1 images (label 0)\n",
        "for root, _, files in os.walk(output_normal):\n",
        "    for file in files:\n",
        "        image_paths.append(os.path.join(root, file))\n",
        "        labels.append(0)\n",
        "\n",
        "# Class-2 images (label 1)\n",
        "for root, _, files in os.walk(output_pneumonia):\n",
        "    for file in files:\n",
        "        image_paths.append(os.path.join(root, file))\n",
        "        labels.append(1)\n",
        "\n",
        "# Class-3 images (label 2)\n",
        "for root, _, files in os.walk(output_tuberculosis):\n",
        "    for file in files:\n",
        "        image_paths.append(os.path.join(root, file))\n",
        "        labels.append(2)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create Dataset and Split into Train/Validation/Test Sets\n",
        "dataset = CustomDataset(image_paths=image_paths, labels=labels)\n",
        "train_size = int(0.72 * len(dataset))  # 80% train-val split -> 90% training from train-val split\n",
        "val_size = int(0.08 * len(dataset))   # 10% validation from train-val split\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "HT9i0g3MEBKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # CORRECTED LINEAR LAYER (calculate actual dimensions)\n",
        "        self.fc1 = nn.Linear(32 * 13 * 13, 32)  # Changed from 4608 to 5408 equivalent\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)  # Removed softmax as CrossEntropyLoss includes it\n",
        "\n",
        "\n",
        "model = CNNModel()\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "qsvTQ4YGB4et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3430jML49r89",
        "outputId": "e345d754-7686-4267-f32d-5ec69b4397f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.553723589900662\n",
            "Validation Loss: 1.1773965656757355, Accuracy: 0.5833\n",
            "Epoch 2/50, Loss: 0.17869891116724296\n",
            "Validation Loss: 0.19228969886898994, Accuracy: 0.9417\n",
            "Epoch 3/50, Loss: 0.13840104743619175\n",
            "Validation Loss: 0.09877778496593237, Accuracy: 0.9750\n",
            "Epoch 4/50, Loss: 0.08214390464127064\n",
            "Validation Loss: 0.26993515715003014, Accuracy: 0.8750\n",
            "Epoch 5/50, Loss: 0.06467841053381562\n",
            "Validation Loss: 0.08230523765087128, Accuracy: 0.9833\n",
            "Epoch 6/50, Loss: 0.03633807718699031\n",
            "Validation Loss: 0.12514756433665752, Accuracy: 0.9500\n",
            "Epoch 7/50, Loss: 0.04317768630297745\n",
            "Validation Loss: 0.2200898751616478, Accuracy: 0.9167\n",
            "Epoch 8/50, Loss: 0.031501638045644056\n",
            "Validation Loss: 0.12414489779621363, Accuracy: 0.9583\n",
            "Epoch 9/50, Loss: 0.02312765601912842\n",
            "Validation Loss: 0.07742186170071363, Accuracy: 0.9667\n",
            "Epoch 10/50, Loss: 0.01311324953156359\n",
            "Validation Loss: 0.06479489430785179, Accuracy: 0.9750\n",
            "Epoch 11/50, Loss: 0.008099660643732505\n",
            "Validation Loss: 0.08804038632661104, Accuracy: 0.9583\n",
            "Epoch 12/50, Loss: 0.003779004105394158\n",
            "Validation Loss: 0.09773256909102201, Accuracy: 0.9750\n",
            "Epoch 13/50, Loss: 0.0028839223086833954\n",
            "Validation Loss: 0.12323219235986471, Accuracy: 0.9667\n",
            "Epoch 14/50, Loss: 0.002515808382989181\n",
            "Validation Loss: 0.10029718652367592, Accuracy: 0.9750\n",
            "Epoch 15/50, Loss: 0.001697889858726686\n",
            "Validation Loss: 0.09640195406973362, Accuracy: 0.9750\n",
            "Epoch 16/50, Loss: 0.0016088607440264348\n",
            "Validation Loss: 0.09938794514164329, Accuracy: 0.9833\n",
            "Epoch 17/50, Loss: 0.001278924781431937\n",
            "Validation Loss: 0.10820135800167918, Accuracy: 0.9750\n",
            "Epoch 18/50, Loss: 0.0011653065628385829\n",
            "Validation Loss: 0.08961984235793352, Accuracy: 0.9750\n",
            "Epoch 19/50, Loss: 0.0010992988219881868\n",
            "Validation Loss: 0.09616491897031665, Accuracy: 0.9833\n",
            "Epoch 20/50, Loss: 0.000931750846468597\n",
            "Validation Loss: 0.10335241723805666, Accuracy: 0.9750\n",
            "Epoch 21/50, Loss: 0.0008113532993079656\n",
            "Validation Loss: 0.12387614976614714, Accuracy: 0.9750\n",
            "Epoch 22/50, Loss: 0.0007583618520156425\n",
            "Validation Loss: 0.10663645947352052, Accuracy: 0.9833\n",
            "Epoch 23/50, Loss: 0.0006915999230627409\n",
            "Validation Loss: 0.10190969426184893, Accuracy: 0.9750\n",
            "Epoch 24/50, Loss: 0.0006407383116311394\n",
            "Validation Loss: 0.11958543071523309, Accuracy: 0.9750\n",
            "Epoch 25/50, Loss: 0.0005316979813334696\n",
            "Validation Loss: 0.11026407685130835, Accuracy: 0.9750\n",
            "Epoch 26/50, Loss: 0.0005056325543966308\n",
            "Validation Loss: 0.1042118645273149, Accuracy: 0.9750\n",
            "Epoch 27/50, Loss: 0.00047814906118968367\n",
            "Validation Loss: 0.11919567221775651, Accuracy: 0.9750\n",
            "Epoch 28/50, Loss: 0.00044097343754187664\n",
            "Validation Loss: 0.1229158928617835, Accuracy: 0.9750\n",
            "Epoch 29/50, Loss: 0.0004278252507741426\n",
            "Validation Loss: 0.12704236386343837, Accuracy: 0.9750\n",
            "Epoch 30/50, Loss: 0.0003819064478353967\n",
            "Validation Loss: 0.11746239196509123, Accuracy: 0.9750\n",
            "Epoch 31/50, Loss: 0.0003675593104751041\n",
            "Validation Loss: 0.11089218547567725, Accuracy: 0.9750\n",
            "Epoch 32/50, Loss: 0.00029567697920891294\n",
            "Validation Loss: 0.12250289600342512, Accuracy: 0.9750\n",
            "Epoch 33/50, Loss: 0.00033198364647379257\n",
            "Validation Loss: 0.11393860122188926, Accuracy: 0.9667\n",
            "Epoch 34/50, Loss: 0.0002827652942334466\n",
            "Validation Loss: 0.12888590851798654, Accuracy: 0.9750\n",
            "Epoch 35/50, Loss: 0.00025265034042497384\n",
            "Validation Loss: 0.12045637331902981, Accuracy: 0.9750\n",
            "Epoch 36/50, Loss: 0.00024152065581053167\n",
            "Validation Loss: 0.12670984212309122, Accuracy: 0.9750\n",
            "Epoch 37/50, Loss: 0.00023733574573056744\n",
            "Validation Loss: 0.12211760738864541, Accuracy: 0.9750\n",
            "Epoch 38/50, Loss: 0.0002385443718487616\n",
            "Validation Loss: 0.12134261522442102, Accuracy: 0.9750\n",
            "Epoch 39/50, Loss: 0.00019964227452446185\n",
            "Validation Loss: 0.12787806056439877, Accuracy: 0.9750\n",
            "Epoch 40/50, Loss: 0.00019723928091801047\n",
            "Validation Loss: 0.12301017576828599, Accuracy: 0.9667\n",
            "Epoch 41/50, Loss: 0.00018380355408333023\n",
            "Validation Loss: 0.12713435804471374, Accuracy: 0.9750\n",
            "Epoch 42/50, Loss: 0.00020849786661154425\n",
            "Validation Loss: 0.12513016536831856, Accuracy: 0.9750\n",
            "Epoch 43/50, Loss: 0.0001640841105922043\n",
            "Validation Loss: 0.1268666093237698, Accuracy: 0.9667\n",
            "Epoch 44/50, Loss: 0.0001900353184229222\n",
            "Validation Loss: 0.1223871516995132, Accuracy: 0.9667\n",
            "Epoch 45/50, Loss: 0.00016926681099207867\n",
            "Validation Loss: 0.13433530880138278, Accuracy: 0.9750\n",
            "Epoch 46/50, Loss: 0.00014915283806176912\n",
            "Validation Loss: 0.12705946015194058, Accuracy: 0.9750\n",
            "Epoch 47/50, Loss: 0.0001436393523523721\n",
            "Validation Loss: 0.1317420545965433, Accuracy: 0.9750\n",
            "Epoch 48/50, Loss: 0.00031140511392175756\n",
            "Validation Loss: 0.12450848298612982, Accuracy: 0.9667\n",
            "Epoch 49/50, Loss: 0.00014651394785152448\n",
            "Validation Loss: 0.13692086888477206, Accuracy: 0.9750\n",
            "Epoch 50/50, Loss: 0.00013973405967750724\n",
            "Validation Loss: 0.13958075176924467, Accuracy: 0.9750\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "num_epochs = 50\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels).item()\n",
        "\n",
        "    val_accuracy = correct_predictions / len(val_dataset)\n",
        "    print(f\"Validation Loss: {val_loss/len(val_loader)}, Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "# Testing Loop with Full Metrics\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate Metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS8X64CR_Atr",
        "outputId": "93429a2e-afad-4f0f-e2d4-b86f5c0d67b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9733\n",
            "Precision: 0.9740\n",
            "Recall: 0.9733\n",
            "F1 Score: 0.9734\n",
            "Cohen's Kappa: 0.9599\n",
            "\n",
            "Confusion Matrix:\n",
            "[[105   5   0]\n",
            " [  0  91   3]\n",
            " [  0   0  96]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1D CNN for 12-lead ECG beat classification"
      ],
      "metadata": {
        "id": "8eqS73wQGSTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "uXsR20uPHDck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.load('/content/drive/MyDrive/x.npy')\n",
        "y=np.load('/content/drive/MyDrive/y.npy')"
      ],
      "metadata": {
        "id": "1HFXyGldGLm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_val, x_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = len(np.unique(y))  # Number of unique classes in y\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val_one_hot = to_categorical(y_val, num_classes=num_classes)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "drvnXsInIZRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes for verification\n",
        "print(\"X_train shape:\", x_train.shape)\n",
        "print(\"X_val shape:\", x_val.shape)\n",
        "print(\"X_test shape:\", x_test.shape)\n",
        "print(\"y_train_one_hot shape:\", y_train_one_hot.shape)\n",
        "print(\"y_val_one_hot shape:\", y_val_one_hot.shape)\n",
        "print(\"y_test_one_hot shape:\", y_test_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uiPb-vXI9k0",
        "outputId": "d8b33898-c29c-46c7-c028-476ccc3ce100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: torch.Size([2520, 12, 650])\n",
            "X_val shape: torch.Size([280, 12, 650])\n",
            "X_test shape: torch.Size([700, 12, 650])\n",
            "y_train_one_hot shape: (2520, 7)\n",
            "y_val_one_hot shape: (280, 7)\n",
            "y_test_one_hot shape: (700, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors with proper dimensions\n",
        "# Original shape: (batch, 650, 12) → needs to become (batch, 12, 650) for PyTorch\n",
        "x_train = torch.tensor(x_train.swapaxes(1, 2), dtype=torch.float32)  # Use swapaxes for 3D arrays\n",
        "x_val = torch.tensor(x_val.swapaxes(1, 2), dtype=torch.float32)  # Use swapaxes for 3D arrays\n",
        "x_test = torch.tensor(x_test.swapaxes(1, 2), dtype=torch.float32)  # Use swapaxes for 3D arrays\n",
        "\n",
        "# Convert one-hot encoded labels back to class indices\n",
        "y_train = torch.tensor(y_train_one_hot.argmax(axis=1), dtype=torch.long) # Corrected variable name\n",
        "y_val = torch.tensor(y_val_one_hot.argmax(axis=1), dtype=torch.long) # Corrected variable name\n",
        "y_test = torch.tensor(y_test_one_hot.argmax(axis=1), dtype=torch.long) # Corrected variable name\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLZJ9mDyGr1k",
        "outputId": "2ddeee50-85fc-4c5a-8e3d-6e4f4abef953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-ffe42cb862ad>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_train = torch.tensor(x_train.swapaxes(1, 2), dtype=torch.float32)  # Use swapaxes for 3D arrays\n",
            "<ipython-input-39-ffe42cb862ad>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_val = torch.tensor(x_val.swapaxes(1, 2), dtype=torch.float32)  # Use swapaxes for 3D arrays\n",
            "<ipython-input-39-ffe42cb862ad>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_test = torch.tensor(x_test.swapaxes(1, 2), dtype=torch.float32)  # Use swapaxes for 3D arrays\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN1D, self).__init__()\n",
        "        # Input: (batch_size, 650 channels, 12 timesteps)\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv1d(650, 16, kernel_size=5),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool1d(kernel_size=2)  # Reduced from 10\n",
        "        )\n",
        "\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool1d(kernel_size=2)  # Reduced from 5\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 2, 16),  # CORRECTED: 32×2=64 → 64→16\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 7)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)  # Output: (batch, 16, 4)\n",
        "        x = self.conv_block2(x)  # Output: (batch, 32, 2)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = CNN1D()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "GpDDE8F1HkHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(100):  # Adjust number of epochs\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Val Loss: {val_loss/len(val_loader):.4f} | Acc: {correct/len(val_dataset):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvb3V-DBHo4H",
        "outputId": "bd9b2cb5-c90e-4eec-e759-f16313f10aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Loss: 0.0592 | Acc: 0.9893\n",
            "Epoch 2 | Val Loss: 0.0753 | Acc: 0.9786\n",
            "Epoch 3 | Val Loss: 0.0539 | Acc: 0.9929\n",
            "Epoch 4 | Val Loss: 0.0608 | Acc: 0.9893\n",
            "Epoch 5 | Val Loss: 0.0567 | Acc: 0.9893\n",
            "Epoch 6 | Val Loss: 0.0621 | Acc: 0.9929\n",
            "Epoch 7 | Val Loss: 0.0578 | Acc: 0.9857\n",
            "Epoch 8 | Val Loss: 0.0540 | Acc: 0.9893\n",
            "Epoch 9 | Val Loss: 0.0524 | Acc: 0.9893\n",
            "Epoch 10 | Val Loss: 0.0627 | Acc: 0.9893\n",
            "Epoch 11 | Val Loss: 0.0587 | Acc: 0.9893\n",
            "Epoch 12 | Val Loss: 0.0669 | Acc: 0.9893\n",
            "Epoch 13 | Val Loss: 0.1031 | Acc: 0.9679\n",
            "Epoch 14 | Val Loss: 0.0705 | Acc: 0.9821\n",
            "Epoch 15 | Val Loss: 0.0726 | Acc: 0.9821\n",
            "Epoch 16 | Val Loss: 0.0523 | Acc: 0.9893\n",
            "Epoch 17 | Val Loss: 0.0609 | Acc: 0.9893\n",
            "Epoch 18 | Val Loss: 0.0565 | Acc: 0.9857\n",
            "Epoch 19 | Val Loss: 0.0650 | Acc: 0.9893\n",
            "Epoch 20 | Val Loss: 0.0659 | Acc: 0.9893\n",
            "Epoch 21 | Val Loss: 0.0688 | Acc: 0.9893\n",
            "Epoch 22 | Val Loss: 0.0658 | Acc: 0.9893\n",
            "Epoch 23 | Val Loss: 0.0668 | Acc: 0.9893\n",
            "Epoch 24 | Val Loss: 0.0742 | Acc: 0.9893\n",
            "Epoch 25 | Val Loss: 0.0756 | Acc: 0.9893\n",
            "Epoch 26 | Val Loss: 0.0773 | Acc: 0.9893\n",
            "Epoch 27 | Val Loss: 0.0784 | Acc: 0.9893\n",
            "Epoch 28 | Val Loss: 0.0677 | Acc: 0.9893\n",
            "Epoch 29 | Val Loss: 0.0787 | Acc: 0.9893\n",
            "Epoch 30 | Val Loss: 0.0763 | Acc: 0.9893\n",
            "Epoch 31 | Val Loss: 0.0771 | Acc: 0.9893\n",
            "Epoch 32 | Val Loss: 0.0763 | Acc: 0.9893\n",
            "Epoch 33 | Val Loss: 0.0808 | Acc: 0.9893\n",
            "Epoch 34 | Val Loss: 0.0836 | Acc: 0.9893\n",
            "Epoch 35 | Val Loss: 0.0822 | Acc: 0.9893\n",
            "Epoch 36 | Val Loss: 0.0809 | Acc: 0.9893\n",
            "Epoch 37 | Val Loss: 0.0848 | Acc: 0.9893\n",
            "Epoch 38 | Val Loss: 0.0772 | Acc: 0.9893\n",
            "Epoch 39 | Val Loss: 0.0919 | Acc: 0.9893\n",
            "Epoch 40 | Val Loss: 0.0861 | Acc: 0.9893\n",
            "Epoch 41 | Val Loss: 0.0912 | Acc: 0.9893\n",
            "Epoch 42 | Val Loss: 0.0892 | Acc: 0.9893\n",
            "Epoch 43 | Val Loss: 0.0861 | Acc: 0.9893\n",
            "Epoch 44 | Val Loss: 0.0841 | Acc: 0.9893\n",
            "Epoch 45 | Val Loss: 0.0862 | Acc: 0.9893\n",
            "Epoch 46 | Val Loss: 0.0849 | Acc: 0.9893\n",
            "Epoch 47 | Val Loss: 0.0877 | Acc: 0.9893\n",
            "Epoch 48 | Val Loss: 0.0868 | Acc: 0.9893\n",
            "Epoch 49 | Val Loss: 0.0873 | Acc: 0.9893\n",
            "Epoch 50 | Val Loss: 0.0887 | Acc: 0.9893\n",
            "Epoch 51 | Val Loss: 0.0907 | Acc: 0.9893\n",
            "Epoch 52 | Val Loss: 0.0888 | Acc: 0.9893\n",
            "Epoch 53 | Val Loss: 0.0882 | Acc: 0.9857\n",
            "Epoch 54 | Val Loss: 0.0906 | Acc: 0.9893\n",
            "Epoch 55 | Val Loss: 0.0932 | Acc: 0.9893\n",
            "Epoch 56 | Val Loss: 0.0902 | Acc: 0.9893\n",
            "Epoch 57 | Val Loss: 0.0722 | Acc: 0.9893\n",
            "Epoch 58 | Val Loss: 0.0833 | Acc: 0.9893\n",
            "Epoch 59 | Val Loss: 0.0555 | Acc: 0.9929\n",
            "Epoch 60 | Val Loss: 0.0560 | Acc: 0.9964\n",
            "Epoch 61 | Val Loss: 0.0530 | Acc: 0.9964\n",
            "Epoch 62 | Val Loss: 0.0658 | Acc: 0.9964\n",
            "Epoch 63 | Val Loss: 0.0557 | Acc: 0.9964\n",
            "Epoch 64 | Val Loss: 0.0745 | Acc: 0.9857\n",
            "Epoch 65 | Val Loss: 0.0654 | Acc: 0.9929\n",
            "Epoch 66 | Val Loss: 0.0740 | Acc: 0.9857\n",
            "Epoch 67 | Val Loss: 0.0908 | Acc: 0.9857\n",
            "Epoch 68 | Val Loss: 0.0746 | Acc: 0.9893\n",
            "Epoch 69 | Val Loss: 0.0805 | Acc: 0.9857\n",
            "Epoch 70 | Val Loss: 0.0807 | Acc: 0.9857\n",
            "Epoch 71 | Val Loss: 0.0766 | Acc: 0.9893\n",
            "Epoch 72 | Val Loss: 0.0789 | Acc: 0.9893\n",
            "Epoch 73 | Val Loss: 0.0804 | Acc: 0.9893\n",
            "Epoch 74 | Val Loss: 0.0792 | Acc: 0.9929\n",
            "Epoch 75 | Val Loss: 0.0781 | Acc: 0.9929\n",
            "Epoch 76 | Val Loss: 0.0769 | Acc: 0.9929\n",
            "Epoch 77 | Val Loss: 0.0790 | Acc: 0.9929\n",
            "Epoch 78 | Val Loss: 0.0783 | Acc: 0.9929\n",
            "Epoch 79 | Val Loss: 0.0810 | Acc: 0.9929\n",
            "Epoch 80 | Val Loss: 0.0775 | Acc: 0.9929\n",
            "Epoch 81 | Val Loss: 0.0808 | Acc: 0.9929\n",
            "Epoch 82 | Val Loss: 0.0770 | Acc: 0.9929\n",
            "Epoch 83 | Val Loss: 0.1260 | Acc: 0.9821\n",
            "Epoch 84 | Val Loss: 0.0698 | Acc: 0.9857\n",
            "Epoch 85 | Val Loss: 0.0632 | Acc: 0.9857\n",
            "Epoch 86 | Val Loss: 0.0621 | Acc: 0.9929\n",
            "Epoch 87 | Val Loss: 0.0622 | Acc: 0.9893\n",
            "Epoch 88 | Val Loss: 0.0628 | Acc: 0.9893\n",
            "Epoch 89 | Val Loss: 0.0618 | Acc: 0.9929\n",
            "Epoch 90 | Val Loss: 0.0648 | Acc: 0.9929\n",
            "Epoch 91 | Val Loss: 0.0569 | Acc: 0.9929\n",
            "Epoch 92 | Val Loss: 0.0570 | Acc: 0.9964\n",
            "Epoch 93 | Val Loss: 0.0626 | Acc: 0.9929\n",
            "Epoch 94 | Val Loss: 0.0631 | Acc: 0.9929\n",
            "Epoch 95 | Val Loss: 0.0604 | Acc: 0.9964\n",
            "Epoch 96 | Val Loss: 0.0629 | Acc: 0.9929\n",
            "Epoch 97 | Val Loss: 0.0621 | Acc: 0.9929\n",
            "Epoch 98 | Val Loss: 0.0606 | Acc: 0.9964\n",
            "Epoch 99 | Val Loss: 0.0619 | Acc: 0.9964\n",
            "Epoch 100 | Val Loss: 0.0651 | Acc: 0.9964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        test_loss += criterion(outputs, labels).item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Store predictions and labels for metrics calculation\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = correct / len(test_dataset)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Optional: Print classification report for more detailed metrics\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ENQC8NOHuIU",
        "outputId": "2525e8ae-8b72-4a1a-9b4f-00317c6246e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0460\n",
            "Accuracy: 0.9929\n",
            "Precision: 0.9930\n",
            "Recall: 0.9929\n",
            "F1 Score: 0.9929\n",
            "Cohen's Kappa: 0.9916\n",
            "\n",
            "Confusion Matrix:\n",
            "[[123   0   0   0   3   0   0]\n",
            " [  0 105   0   0   0   0   0]\n",
            " [  0   0  95   0   0   0   0]\n",
            " [  0   0   0 101   0   0   0]\n",
            " [  0   1   0   0  86   0   0]\n",
            " [  0   0   0   0   0  88   1]\n",
            " [  0   0   0   0   0   0  97]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.9762    0.9880       126\n",
            "           1     0.9906    1.0000    0.9953       105\n",
            "           2     1.0000    1.0000    1.0000        95\n",
            "           3     1.0000    1.0000    1.0000       101\n",
            "           4     0.9663    0.9885    0.9773        87\n",
            "           5     1.0000    0.9888    0.9944        89\n",
            "           6     0.9898    1.0000    0.9949        97\n",
            "\n",
            "    accuracy                         0.9929       700\n",
            "   macro avg     0.9924    0.9934    0.9928       700\n",
            "weighted avg     0.9930    0.9929    0.9929       700\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjRSp2ceLKVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}